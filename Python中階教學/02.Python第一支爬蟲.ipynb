{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python中階-第一支爬蟲程式\n",
    "\n",
    "\n",
    "## 中正大學資管系 (20181021) 大綱\n",
    "\n",
    "這部分會說明如何完成你的第一支爬蟲程式，接著將這支程式結構化\n",
    "\n",
    "+ 起手式\n",
    "+ 分析網站與內容\n",
    "+ 爬你所見\n",
    "+ 爬你所想\n",
    "+ 結構化程式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 起手式\n",
    "+ `import requests` 這個套件\n",
    "+ 建立一個 `resp` 變數去儲存 `requests` 抓到的網頁內容\n",
    "+ `resp` 的變數除了內容外也會回傳一個 html 狀態碼\n",
    "\n",
    "通常這一部成功代表網頁並未針對爬蟲做任何阻擋，有些網站會基於某些理由讓別人爬他的網站，例如: 購物網站(蝦皮)，或是擁有大量資料的網站，例如股票訊息，這些網站防止爬蟲理由可能基於避免大量爬蟲而影響真正使用者的使用感受(速度變慢等等)\n",
    "\n",
    "但這邊我們都不先談這些複雜的破解阻擋方法(可能某些也無法破解)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://www.ptt.cc/bbs/movie/index.html'\n",
    "resp = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Response [200]>, 200)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp, resp.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分析網站內容與結構\n",
    "\n",
    "網站的內容組成不外乎就是 html + css，這些構成的結構通常我們會稱為標籤（就是前者的語法，有這些語法才能形成類似表格、表單等等）\n",
    "在爬蟲之前其實最困難的是理解這些結構。要分析這個結構通常會推薦使用 Google Chrome 瀏覽器的`開發人員功能`，開啟方法就是在網頁裡點選右鍵，找到`檢查功能`點選就會看到如下圖(版面調整可從檢查功能右上角選單做改變)，或是你也可以按 F12 快速鍵開啟\n",
    "\n",
    "![](images/CHROME.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬你所見\n",
    "\n",
    "以 ptt 網頁版為例子，整個版面是在 `<body>..</body>` 標籤內，但實際讓每篇文章又被歸類在 `<div class='r-ent'></div>` 下懂了！所以如果我要爬文章，最簡單方法就是去找出所有標籤是 `<div class='r-ent'></div>` 的內容就好了對吧？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests_html import HTML\n",
    "html = HTML(html=resp.text)\n",
    "post_entries = html.find('div.r-ent')\n",
    "post_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "post_entries 會使用一個 `list(串列)`儲存爬取頁面的所有的文章，印出來你看到的會是作者將這些 html, css 的檔案內容包裝成某個物件的型態(可以比對 `BeautifulSoup` 並未包裝)。但現在會有個疑問，除了看文件了解要怎樣取出物件型態的內容，還有什麼方法可以知道呢？\n",
    "\n",
    "還記得基礎課程所教的 `dir()` ? 讓它派上用場吧 dir(post_entries[0]) 馬上可以看到他提供很多功能讓我們能再進一步取得每篇文章底下的訊息。但這時會發現，底下的內容也是一連串的元素，這些元素各自有標籤存放著訊息，繼續用`開發人員工具`看看？接著把 post_entries 內的值印出來確認是否一樣？\n",
    "\n",
    "![#](images/CHROME2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[公告] 水桶公告 20181017'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent1 = post_entries[0] # 先試試第一片是否為公告\n",
    "ent1.find('div.title', first=True).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先為什麼 ent1.find() 要在第二個參數放 `first=True`? 印為這個套件作者在[文件上說明](https://html.python-requests.org/)，如果要爬取的是一個 [css selector](https://www.w3schools.com/cssref/css_selectors.asp) 那使用 `.find` 這個方法就需要加這個參數\n",
    "\n",
    "如果是要爬取 `<div><title></div>` 下的超連結，那就需要透過 `css selector` 語法去往下搜尋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/bbs/movie/M.1539741182.A.7CD.html'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent1.find('div.title > a', first=True).attrs['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/bbs/movie/M.1539741182.A.7CD.html'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent1.find('div.title a', first=True).attrs['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬你想要\n",
    "\n",
    "當瞭解一個網站結構之後接著就可以開始嘗試寫邏輯把內容爬出，接著我們就來示範一下:\n",
    "+ 首先會使用一個 `for loop` 將存在 `post_entries` 這個 list 的資料結構內容讀出來 \n",
    "+ 接著會建立一個 context 的 dict，根據 **title, push, date, author** 做 key 存起來\n",
    "+ 最後印出內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '[公告] 水桶公告 20181017', 'push': '1', 'date': '10/17', 'author': 'VOT1077'}\n",
      "{'title': '[新聞] 登陸月球50年《電影哆啦A夢 大雄的月球探', 'push': '1', 'date': '10/17', 'author': 'hoanbeh'}\n",
      "{'title': 'Re: [贈票] 【極智對決】 台北贈票', 'push': '10', 'date': '10/17', 'author': 'rapsd520'}\n",
      "{'title': '新聞文章請以新發文方式-V <Reewalker>', 'push': '', 'date': '10/17', 'author': '-'}\n",
      "{'title': '[請益] 李小龍傳', 'push': '4', 'date': '10/17', 'author': 'hsinofkids'}\n",
      "{'title': '[討論] 最經典的系列作有哪些', 'push': '3', 'date': '10/17', 'author': 'assggy'}\n",
      "{'title': '[版規] 電影版版規 201808', 'push': '1', 'date': '8/28', 'author': 'VOT1077'}\n",
      "{'title': '[公告] 電影版板規修訂說明', 'push': '', 'date': '8/28', 'author': 'VOT1077'}\n",
      "{'title': '[公告] 關於特定影片負(好)雷討論', 'push': '33', 'date': '10/11', 'author': 'VOT1077'}\n"
     ]
    }
   ],
   "source": [
    "for entry in post_entries:\n",
    "    context = {\n",
    "        'title': entry.find('div.title', first=True).text,\n",
    "        'push': entry.find('div.nrec', first=True).text,\n",
    "        'date': entry.find('div.date', first=True).text,\n",
    "        'author': entry.find('div.author', first=True).text,\n",
    "    }\n",
    "    print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 程式碼目前應該會長成這樣\n",
    "```python\n",
    "import requests\n",
    "from requests_html import HTML\n",
    "\n",
    "url = 'https://www.ptt.cc/bbs/movie/index.html'\n",
    "resp = requests.get(url)\n",
    "html = HTML(html=resp.text)\n",
    "post_entries = html.find('div.r-ent')\n",
    "\n",
    "for entry in post_entries:\n",
    "    context = {\n",
    "        'title': entry.find('div.title', first=True).text,\n",
    "        'push': entry.find('div.nrec', first=True).text,\n",
    "        'date': entry.find('div.date', first=True).text,\n",
    "        'author': entry.find('div.author', first=True).text,\n",
    "    }\n",
    "    print(context)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結構化程式\n",
    "\n",
    "到目前為止雖然成功完成了第一個爬蟲程式，但通常會認為要將程式做結構化的調整，意思是如果將程式碼從頭寫到尾不做結構化，通常許多類似功能會重複的出現，這不但為讓程式碼看起來冗長且維護起來不容易\n",
    "\n",
    "因此我們先來看看哪些部分是有可能是可以作為修改的？\n",
    "\n",
    "+ 找出可以作為變數的部分\n",
    "+ 將可能重複使用的部分轉變成函式\n",
    "\n",
    "看起來 `url` 這個變數與 `requests.get(url)` 這個方法在使用這支函式時都會使用到，而且會改變部分只有 `url` 那先試著把這邊做結構化動作 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(url):\n",
    "    response = requests.get(url)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = fetch(url='https://www.ptt.cc/bbs/movie/index.html')\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "成功完成第一步，接著我們來思考\n",
    "+ 當取的內容之後會經過一層處理接著回傳給我們想要的資料結構，但有可能這部分未必是每次內容都要相同，或許把其升級為函式彈性也會更好些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_article_meta(entry):\n",
    "    context = {\n",
    "        'title': entry.find('div.title', first=True).text,\n",
    "        'push': entry.find('div.nrec', first=True).text,\n",
    "        'date': entry.find('div.date', first=True).text,\n",
    "        'author': entry.find('div.author', first=True).text,\n",
    "    }\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前已經把兩個部分升級為函式，通常我們可能會把流程放在一個 `main()` 去進行處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '[公告] 水桶公告 20181017', 'push': '2', 'date': '10/17', 'author': 'VOT1077'}\n",
      "{'title': '[新聞] 登陸月球50年《電影哆啦A夢 大雄的月球探', 'push': '3', 'date': '10/17', 'author': 'hoanbeh'}\n",
      "{'title': 'Re: [贈票] 【極智對決】 台北贈票', 'push': '14', 'date': '10/17', 'author': 'rapsd520'}\n",
      "{'title': '新聞文章請以新發文方式-V <Reewalker>', 'push': '', 'date': '10/17', 'author': '-'}\n",
      "{'title': '[請益] 李小龍傳', 'push': '4', 'date': '10/17', 'author': 'hsinofkids'}\n",
      "{'title': '[討論] 最經典的系列作有哪些', 'push': '4', 'date': '10/17', 'author': 'assggy'}\n",
      "{'title': '[新聞] 重建熱蘭遮城將投入135億 魏德聖：2024', 'push': '48', 'date': '10/17', 'author': 'purue'}\n",
      "{'title': '[贈票] 極智對決 週六台北贈票', 'push': '', 'date': '10/17', 'author': 'WAV'}\n",
      "{'title': '[情報] 2018 亞太銀幕獎 入圍名單', 'push': '', 'date': '10/17', 'author': 'qpr322'}\n",
      "{'title': '[問片] 孕婦車禍流產，找人復仇的血腥片', 'push': '1', 'date': '10/17', 'author': 'shuffling'}\n",
      "{'title': '[新聞] 華倫夫婦加入《安娜貝爾3》', 'push': '1', 'date': '10/17', 'author': 'shengchiu303'}\n",
      "{'title': '[版規] 電影版版規 201808', 'push': '1', 'date': '8/28', 'author': 'VOT1077'}\n",
      "{'title': '[公告] 電影版板規修訂說明', 'push': '', 'date': '8/28', 'author': 'VOT1077'}\n",
      "{'title': '[公告] 關於特定影片負(好)雷討論', 'push': '33', 'date': '10/11', 'author': 'VOT1077'}\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    resp = fetch(url='https://www.ptt.cc/bbs/movie/index.html')\n",
    "    if resp.status_code == 200:\n",
    "        html = HTML(html=resp.text)\n",
    "        post_entries = html.find('div.r-ent')\n",
    "\n",
    "        for entry in post_entries:\n",
    "            meta = parser_article_meta(entry)\n",
    "            print(meta)\n",
    "    else:\n",
    "        print(resp.status_code)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著來看一起我們的程式目前會長這樣 \n",
    "\n",
    "```python\n",
    "#-*- coding: utf-8 -*-\n",
    "import requests\n",
    "from requests_html import HTML\n",
    "\n",
    "\n",
    "def fetch(url):\n",
    "    response = requests.get(url)\n",
    "    return response\n",
    "\n",
    "\n",
    "def parser_article_meta(entry):\n",
    "    context = {\n",
    "        'title': entry.find('div.title', first=True).text,\n",
    "        'push': entry.find('div.nrec', first=True).text,\n",
    "        'date': entry.find('div.date', first=True).text,\n",
    "        'author': entry.find('div.author', first=True).text,\n",
    "    }\n",
    "    return context\n",
    "\n",
    "\n",
    "def main():\n",
    "    resp = fetch(url='https://www.ptt.cc/bbs/movie/index.html')\n",
    "    if resp.status_code == 200:\n",
    "        html = HTML(html=resp.text)\n",
    "        post_entries = html.find('div.r-ent')\n",
    "\n",
    "        for entry in post_entries:\n",
    "            meta = parser_article_meta(entry)\n",
    "            print(meta)\n",
    "    else:\n",
    "        print(resp.status_code)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把它存檔吧！你已經完成了第一個爬蟲程式了，有沒有好棒棒！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
